PoseNet-Pytorch를 활용한 Visual 기반 Localization

ROS상에서 사용이 가능한 node 제작
- 지금까지 사용한 오픈소스 및 모델들을 활용하여 Visual Localization Node 제작
- Test dataset을 실행(rosrun python3 test.py ..)시켜 실시간으로 GT의 pose정보와 Predict한 pose정보를 Rviz상에서 시각화하기(interim_report solver.py:test() 참고)

1) 설명란에 있는 given_rosgraph.png 파악: Hide debug check 풀면 /rviz 보임
4개의 노드 생성: image_node, posenet_node, data_publish_node, rviz_node
* image_node: image publisher ( in solver.py using pos_out? )
* posenet_node: image subscriber & predicted-pose publisher ( new file )
* data_publish_node: GT-pose publisher ( new file using `from solver import test` )
* rviz_node: pose(predicted, GT)-subscriber & marker(predicted, GT)-publisher ( new file )

step 1. rviz_node에서 rviz로 train_marker, test_marker publish해보기

----build-----

$ cd ~/catkin_ws/src
$ catkin_create_pkg slam rospy std_msgs sensor_msgs geometry_msgs visualization_msgs
~/catkin_ws/src/slam/src 위치에 학습 완료된 posenet-pytorch 그대로 옮기기
CMake 수정
test.py 맨 위에 #!/usr/bin/env python3 추가
solver.py solver() 수정
$ cd ~/catkin_ws && catkin_make --only-pkg-with-deps slam


---Run Test dataset---
$ roslaunch lego_loam run.launch
$ cd ~/catkin_ws/src/slam/src
$ rosrun slam test.py --image_path ./data --metadata_path ./data/dataset_test.txt